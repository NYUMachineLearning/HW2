\documentclass[11pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Regression},
            pdfauthor={Anna Yeaton},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Regression}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Anna Yeaton}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{Fall 2019}


\begin{document}
\maketitle

\#Lab Section

In this lab, we will go over regression. We will be using the caret
package in R.
\url{https://topepo.github.io/caret/train-models-by-tag.html}

\hypertarget{perfomance-metrics}{%
\section{Perfomance Metrics}\label{perfomance-metrics}}

\hypertarget{residual}{%
\subsection{Residual}\label{residual}}

Deviation of the observed value to the estimated value (sample mean)
\[residual=y_i - \hat{y_i}\] where \(\hat{y_i}\) is the estimated value

\hypertarget{mean-squared-error-mse}{%
\subsection{Mean Squared Error (MSE)}\label{mean-squared-error-mse}}

\[MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2\]

\hypertarget{root-mean-squared-error-rmse}{%
\subsection{Root Mean Squared Error
(RMSE)}\label{root-mean-squared-error-rmse}}

Same units as original data.

\[RMSE=\sqrt{MSE}\]

\hypertarget{l2-regularization-ridge-regression.-regularize-by-adding-the-sum-of-the-coefficients-squared-to-the-function.}{%
\subsection{L2 regularization : Ridge regression. Regularize by adding
the sum of the coefficients, squared, to the
function.}\label{l2-regularization-ridge-regression.-regularize-by-adding-the-sum-of-the-coefficients-squared-to-the-function.}}

\[Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2\]

\hypertarget{l1-regularization-lasso-regression.-regularize-by-adding-the-sum-of-the-absolute-value-of-the-coefficients-to-the-model.-coefficient-estimates-may-be-pushed-to-zero-lasso-can-perform-variable-selection}{%
\subsection{L1 regularization : Lasso Regression. Regularize by adding
the sum of the absolute value of the coefficients to the model.
Coefficient estimates may be pushed to zero -- Lasso can perform
variable
selection}\label{l1-regularization-lasso-regression.-regularize-by-adding-the-sum-of-the-absolute-value-of-the-coefficients-to-the-model.-coefficient-estimates-may-be-pushed-to-zero-lasso-can-perform-variable-selection}}

\[Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|\]

\newpage

\hypertarget{the-broad-steps-of-machine-learning-in-r.}{%
\subsubsection{The broad steps of Machine learning in
R.}\label{the-broad-steps-of-machine-learning-in-r.}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Split the data into training and test. Set test aside.
\item
  Fit a good model to the training data.
\item
  Visualize if your model learned on the training data.
\item
  Test how your model performs on the test data.
\end{enumerate}

\hypertarget{regression}{%
\section{Regression}\label{regression}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Split data into training and test set (75\% in train set, 25\% in test
  set)
\end{enumerate}

\hypertarget{linear-regression}{%
\subsubsection{Linear Regression}\label{linear-regression}}

\begin{itemize}
\tightlist
\item
  Assumes a linear relationship.
\item
  Independent variables should not be correlated (no mulitcollinearity)
\item
  The number of observations should be greater than the number of
  independent variables.
\end{itemize}

\[RSS=\sum(y_i - \hat{y_i})^2\] We will predict the response of the
Temperature based on Wind.

This is the data we will fit a linear model to.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# ggplot(data = train_regression) +}
\CommentTok{#   geom_point(aes(x=Wind, y=Temp)) +}
\CommentTok{#   theme_bw()}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Create and fit a linear model to predict Temperature from Wind using
  the training set
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#help(train)}

\CommentTok{#linear_regression <- train( ~ , data= , method = "lm")}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  Vizualize how your model performed on the train data by plotting the
  regression line on top of the train data points.
\item
  Explore how the model performs on the test data
\end{enumerate}

\begin{itemize}
\tightlist
\item
  The residuals should be close to zero.
\item
  There should be equal variance around the regression line
  (homoscedasticity).
\item
  Residuals should be normally distributed.
\item
  Independent variables and residuals should not be correlated.
\end{itemize}

4 a) See how the model performs on the test data

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#help(predict)}
\CommentTok{#linear_predict <- predict(, newdata=)}
\end{Highlighting}
\end{Shaded}

4 b) Look at the residuals. Are they close to zero?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#look at the median residual value. Close to zero is best}
\CommentTok{#help(summary)}
\end{Highlighting}
\end{Shaded}

4 c) Plot predicted temperature vs observed temperature. A strong model
should show a strong correlation

4 d) Visualize the regression line in relation to the read data points.
Look for homoscedasticity

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Extract coefficients from the model}

\CommentTok{# plot the regression line on the predicted values}

\CommentTok{# plot the original test values}
\end{Highlighting}
\end{Shaded}

4 e) Residuals should be normally distributed. Plot the density of the
residuals

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#residuals_lin <- residuals(linear_regression)}

\CommentTok{#ggplot(data=residvpredict) +}
\CommentTok{#  geom_density(aes(residual))}
\end{Highlighting}
\end{Shaded}

4 f) Independent variables and residuals should not be correlated

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#cor.test(train_regression$Wind, resid(linear_regression))}
\end{Highlighting}
\end{Shaded}

\hypertarget{linear-regression-with-regularization}{%
\subsubsection{Linear Regression with
Regularization}\label{linear-regression-with-regularization}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Create a linear model using L1 or L2 regularization to predict
  Temperature from Wind and Month variables.
\end{enumerate}


\end{document}
